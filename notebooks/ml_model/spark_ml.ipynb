{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8c82d4b5-3832-42e8-ae12-280e8cf54033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- trans_date_trans_time: timestamp (nullable = true)\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- merchant: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- amt: double (nullable = true)\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zip: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- city_pop: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- dob: date (nullable = true)\n",
      " |-- trans_num: string (nullable = true)\n",
      " |-- unix_time: integer (nullable = true)\n",
      " |-- merch_lat: double (nullable = true)\n",
      " |-- merch_long: double (nullable = true)\n",
      " |-- is_fraud: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, datediff, current_date\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "import math\n",
    "from hdfs import InsecureClient\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "# Start Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FraudDetectionSparkML\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load data\n",
    "\n",
    "    \n",
    "# Haversine formula as a Spark UDF\n",
    "def haversine(lat, lon, merch_lat, merch_lon):\n",
    "    R = 6371.0\n",
    "    lat1 = math.radians(lat)\n",
    "    lon1 = math.radians(lon)\n",
    "    lat2 = math.radians(merch_lat)\n",
    "    lon2 = math.radians(merch_lon)\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"trans_date_trans_time\", StringType(), True),\n",
    "    StructField(\"cc_num\", DoubleType(), True),\n",
    "    StructField(\"merchant\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"amt\", DoubleType(), True),\n",
    "    StructField(\"first\", StringType(), True),\n",
    "    StructField(\"last\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"street\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"zip\", DoubleType(), True),\n",
    "    StructField(\"lat\", DoubleType(), True),\n",
    "    StructField(\"long\", DoubleType(), True),\n",
    "    StructField(\"city_pop\", DoubleType(), True),\n",
    "    StructField(\"job\", StringType(), True),\n",
    "    StructField(\"dob\", StringType(), True),\n",
    "    StructField(\"trans_num\", StringType(), True),\n",
    "    StructField(\"unix_time\", DoubleType(), True),\n",
    "    StructField(\"merch_lat\", DoubleType(), True),\n",
    "    StructField(\"merch_long\", DoubleType(), True),\n",
    "    StructField(\"age\", DoubleType(), True),\n",
    "    StructField(\"distance\", DoubleType(), True),\n",
    "    StructField(\"is_fraud\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Spark ML stages\n",
    "df = spark.read.csv(\n",
    "    \"hdfs://hadoop-namenode:9000/data\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    "    # schema=schema\n",
    ")\n",
    "\n",
    "df.printSchema()\n",
    "# Save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "596ff054-4129-4285-a211-e71ec6ea7b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+------+--------+--------+---+-----+----+------+------+----+-----+---+---+----+--------+---+---+---------+---------+---------+----------+--------+\n",
      "|_c0|trans_date_trans_time|cc_num|merchant|category|amt|first|last|gender|street|city|state|zip|lat|long|city_pop|job|dob|trans_num|unix_time|merch_lat|merch_long|is_fraud|\n",
      "+---+---------------------+------+--------+--------+---+-----+----+------+------+----+-----+---+---+----+--------+---+---+---------+---------+---------+----------+--------+\n",
      "|  0|                    0|     0|       0|       0|  0|    0|   0|     0|     0|   0|    0|  0|  0|   0|       0|  0|  0|        0|        0|        0|         0|       0|\n",
      "+---+---------------------+------+--------+--------+---+-----+----+------+------+----+-----+---+---+----+--------+---+---+---------+---------+---------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "null_counts = df.select([spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f8723dc8-4f5b-4b48-9168-03fc3789c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "haversine_udf = udf(haversine, DoubleType())\n",
    "\n",
    "# Feature engineering\n",
    "df = df.withColumn(\"dob\", col(\"dob\").cast(\"date\"))\n",
    "df = df.withColumn(\"age\", (datediff(current_date(), col(\"dob\")) / 365.25).cast(IntegerType()))\n",
    "df = df.withColumn(\"distance\", haversine_udf(col(\"lat\"), col(\"long\"), col(\"merch_lat\"), col(\"merch_long\")))\n",
    "\n",
    "# Drop unused columns\n",
    "drop_cols = [\"Unnamed: 0\", \"trans_date_trans_time\", \"trans_num\", \"dob\", \"unix_time\",\n",
    "             \"lat\", \"long\", \"merch_lat\", \"merch_long\", \"first\", \"last\"]\n",
    "df = df.drop(*drop_cols)\n",
    "\n",
    "fraud_df = df.filter(col(\"is_fraud\") == 1)\n",
    "nonfraud_df = df.filter(col(\"is_fraud\") == 0).sample(fraction=fraud_df.count() / df.filter(col(\"is_fraud\") == 0).count(), seed=42)\n",
    "df_balanced = fraud_df.union(nonfraud_df)\n",
    "\n",
    "# Categorical & numeric columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9373b620-4457-4aa4-b08d-aca5c87c1403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------+---+------+------+----+-----+---+--------+---+--------+---+--------+\n",
      "|cc_num|merchant|category|amt|gender|street|city|state|zip|city_pop|job|is_fraud|age|distance|\n",
      "+------+--------+--------+---+------+------+----+-----+---+--------+---+--------+---+--------+\n",
      "|     0|       0|       0|  0|     0|     0|   0|    0|  0|       0|  0|       0|  0|       0|\n",
      "+------+--------+--------+---+------+------+----+-----+---+--------+---+--------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "null_counts = df_balanced.select([spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8dd8a390-1d08-48c5-986e-72a2379825a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7506\n",
      "7544\n"
     ]
    }
   ],
   "source": [
    "print(df_balanced.filter(col(\"is_fraud\") == 1).count())\n",
    "print(df_balanced.filter(col(\"is_fraud\") == 0).count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "68f94081-5734-450c-8023-6dd2d877ffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categorical_cols = [field.name for field in df.schema.fields \n",
    "                    if isinstance(field.dataType, StringType)]\n",
    "\n",
    "numeric_cols = [field.name for field in df.schema.fields \n",
    "                if not isinstance(field.dataType, StringType) \n",
    "                and field.name != \"is_fraud\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "28663158-55d0-4c98-8c03-34b30f9840c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['merchant', 'category', 'gender', 'street', 'city', 'state', 'job']\n",
      "['cc_num', 'amt', 'zip', 'city_pop', 'age', 'distance']\n"
     ]
    }
   ],
   "source": [
    "print(categorical_cols)\n",
    "print(numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "dc6b24cc-d584-49c6-b287-5b894a293869",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = df_balanced.drop(\"_c0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c56cac06-c4e5-4e9b-8b0b-621ff5beb173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- merchant: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- amt: double (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zip: integer (nullable = true)\n",
      " |-- city_pop: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- is_fraud: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- distance: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_balanced.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "40c55cbe-9ef6-4716-9daf-41ec838744b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['merchant', 'category', 'gender', 'street', 'city', 'state', 'job']\n",
      "['cc_num', 'amt', 'zip', 'city_pop', 'age', 'distance']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5170ef88-f466-4e26-a82b-5d106b23fad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Drop nulls only from numeric columns\n",
    "# StringIndexer + OneHotEncoder for categorical columns\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=colname, outputCol=colname + \"_index\", handleInvalid=\"keep\")\n",
    "    for colname in categorical_cols\n",
    "]\n",
    "\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=colname + \"_index\", outputCol=colname + \"_ohe\" , handleInvalid=\"keep\")\n",
    "    for colname in categorical_cols\n",
    "]\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[col + \"_ohe\" for col in categorical_cols] + numeric_cols,\n",
    "    outputCol=\"features\" , handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# Classifier\n",
    "classifier = GBTClassifier(labelCol=\"is_fraud\", featuresCol=\"features\", maxIter=50)\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, classifier])\n",
    "\n",
    "# Train/test split\n",
    "train_df, test_df = df_balanced.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5853bda4-03e7-4e74-9146-8eb7c9890c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0b40e0d1-a162-4d73-8498-5e8e11477e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------------------------------+\n",
      "|is_fraud|prediction|probability                              |\n",
      "+--------+----------+-----------------------------------------+\n",
      "|1       |1.0       |[0.037192793436581766,0.9628072065634182]|\n",
      "|1       |1.0       |[0.29603400128229684,0.7039659987177032] |\n",
      "|1       |1.0       |[0.10274545613195854,0.8972545438680415] |\n",
      "|1       |1.0       |[0.027282491075663655,0.9727175089243364]|\n",
      "|1       |1.0       |[0.027282491075663655,0.9727175089243364]|\n",
      "+--------+----------+-----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# Evaluate\n",
    "predictions = model.transform(test_df)\n",
    "predictions.select(\"is_fraud\", \"prediction\", \"probability\").show(5, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "62cc9255-b3fe-417c-a0fb-c9a1d50c37cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9625779625779626\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"is_fraud\", metricName=\"accuracy\")\n",
    "print(\"Test Accuracy:\", evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f0aadf61-6d3c-4873-a476-fbc5565c3862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----+\n",
      "|is_fraud|prediction|count|\n",
      "+--------+----------+-----+\n",
      "|       1|       0.0|   59|\n",
      "|       1|       1.0| 1381|\n",
      "|       0|       0.0| 1397|\n",
      "|       0|       1.0|   49|\n",
      "+--------+----------+-----+\n",
      "\n",
      "+--------+----+----+\n",
      "|is_fraud| 0.0| 1.0|\n",
      "+--------+----+----+\n",
      "|       1|  59|1381|\n",
      "|       0|1397|  49|\n",
      "+--------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "# Compute confusion matrix\n",
    "confusion_df = predictions.groupBy(\"is_fraud\", \"prediction\").agg(count(\"*\").alias(\"count\"))\n",
    "confusion_df.show()\n",
    "\n",
    "# Optional: pivot to make it look like a 2x2 matrix\n",
    "confusion_matrix = confusion_df.groupBy(\"is_fraud\").pivot(\"prediction\").sum(\"count\").fillna(0)\n",
    "confusion_matrix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "372aad92-885b-4b97-8954-c170fc1e5d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"saved_model/fraud_detection_sparkml_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40536ac8-43f5-4b47-8ffe-4bd85e742dea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
