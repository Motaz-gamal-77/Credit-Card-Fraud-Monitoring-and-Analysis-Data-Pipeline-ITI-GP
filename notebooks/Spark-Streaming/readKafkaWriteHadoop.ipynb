{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "768afd20-cce3-46e0-99d1-bfe84aec3c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "932a0ed7-09cd-4d68-877f-10137e59bc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaToHDFS\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd75e475-68ad-4e62-aea1-7c2104a5ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df  = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka_v2:9092\") \\\n",
    "    .option(\"subscribe\", \"credit_card_trans\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "595cdf9b-24a9-4edb-90be-5321e0ccdeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2a13ea2-d631-4ec1-9b81-6742d09e51dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = kafka_df.selectExpr(\"CAST(value AS STRING) as json_str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5314fc7-b07a-481a-aedb-b1c7156be896",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"\", StringType(), True),\n",
    "    StructField(\"trans_date_trans_time\", StringType(), True),\n",
    "    StructField(\"cc_num\", StringType(), True),\n",
    "    StructField(\"merchant\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"amt\", StringType(), True),\n",
    "    StructField(\"first\", StringType(), True),\n",
    "    StructField(\"last\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"street\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"zip\", StringType(), True),\n",
    "    StructField(\"lat\", StringType(), True),\n",
    "    StructField(\"long\", StringType(), True),\n",
    "    StructField(\"city_pop\", StringType(), True),\n",
    "    StructField(\"job\", StringType(), True),\n",
    "    StructField(\"dob\", StringType(), True),\n",
    "    StructField(\"trans_num\", StringType(), True),\n",
    "    StructField(\"unix_time\", StringType(), True),\n",
    "    StructField(\"merch_lat\", StringType(), True),\n",
    "    StructField(\"merch_long\", StringType(), True),\n",
    "    StructField(\"is_fraud\", StringType(), True),\n",
    "    StructField(\"event_time\",StringType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebf61a5e-3fe4-4eb6-967c-369a7757b975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gracefully stopping the stream...\n"
     ]
    }
   ],
   "source": [
    "query = df.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"checkpointLocation\", \"hdfs://hadoop-namenode:9000/user/jovyan/checkpoints/kafka_hdfs_job\") \\\n",
    "    .option(\"path\", \"hdfs://hadoop-namenode:9000/user/jovyan/output/kafka_parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime=\"1 hour\") \\\n",
    "    .start()\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Gracefully stopping the stream...\")\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cc8116fb-49f2-4121-981c-e951f39d9e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to check that data lake works \n",
    "\n",
    "# from hdfs import InsecureClient\n",
    "\n",
    "# client = InsecureClient('http://hadoop-namenode:9870', user='jovyan')\n",
    "\n",
    "# try:\n",
    "#     files = client.list('/user/jovyan/output/kafka_parquet')\n",
    "#     print(\"Folder exists. Files:\")\n",
    "#     for f in files:\n",
    "#         print(\" -\", f)\n",
    "# except Exception as e:\n",
    "#     print(\"Folder not found yet or no files written.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "737aa5af-4e47-4bc3-98ee-13bde3b58814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete created folder in HDFS\n",
    "\n",
    "# from hdfs import InsecureClient\n",
    "\n",
    "# client = InsecureClient('http://hadoop-namenode:9870', user='jovyan')\n",
    "\n",
    "# target_path = '/user/jovyan/output/kafka_parquet'\n",
    "\n",
    "# try:\n",
    "#     if client.status(target_path, strict=False):\n",
    "#         client.delete(target_path, recursive=True)\n",
    "#         print(f\"Deleted HDFS folder: {target_path}\")\n",
    "#     else:\n",
    "#         print(f\" Folder does not exist: {target_path}\")\n",
    "# except Exception as e:\n",
    "#     print(f\" Error deleting folder: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bbc82b-86d7-416c-bc76-4211bdf06475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
